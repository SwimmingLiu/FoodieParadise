"""
Agent æœåŠ¡æ¨¡å—

å®ç°åŸºäº LangGraph çš„æ™ºèƒ½ Agent å·¥ä½œæµã€‚
åŒ…å«"å»å“ªåƒ"ã€"æŸ¥é¢„åˆ¶"ã€"åƒå¤šå°‘"ä¸‰ä¸ªåŠŸèƒ½çš„å®Œæ•´æµç¨‹ã€‚
"""

from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
import json
import base64
import re
import os
import random

from app.models.state import AgentState
from app.services.tools import search_location, analyze_premade, analyze_calories
from app.constants.prompts import WHERE_TO_EAT_PROMPT
from app.constants.preset_responses import (
    WHERE_TO_EAT_PRESETS,
    CHECK_PREMADE_PRESETS,
    CALORIES_PRESETS
)
from app.config import settings
from app.utils.stream_utils import ContentSplitter

from langchain_core.runnables import RunnableConfig
from langchain_core.callbacks.manager import adispatch_custom_event


# ========== è¾…åŠ©å‡½æ•° ==========

def get_preset_response(preset_list: list) -> str:
    """ä»é¢„è®¾å“åº”åˆ—è¡¨ä¸­éšæœºé€‰æ‹©ä¸€æ¡
    
    ç”¨äºåœ¨LLMå®é™…å“åº”å‰ç»™ç”¨æˆ·æä¾›å³æ—¶åé¦ˆï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚
    æ¯æ¬¡è°ƒç”¨éšæœºé€‰æ‹©ï¼Œç¡®ä¿ç”¨æˆ·çœ‹åˆ°çš„æç¤ºæ–‡æœ¬æ¯æ¬¡éƒ½ä¸åŒã€‚
    
    Args:
        preset_list: é¢„è®¾å“åº”æ–‡æœ¬åˆ—è¡¨
        
    Returns:
        str: éšæœºé€‰æ‹©çš„é¢„è®¾å“åº”æ–‡æœ¬
    """
    return random.choice(preset_list)


def encode_image(image_path):
    """å°†å›¾ç‰‡æ–‡ä»¶ç¼–ç ä¸ºBase64å­—ç¬¦ä¸²
    
    è¯»å–æœ¬åœ°å›¾ç‰‡æ–‡ä»¶å¹¶è½¬æ¢ä¸ºBase64ç¼–ç ï¼Œç”¨äºåœ¨APIè¯·æ±‚ä¸­ä¼ è¾“å›¾ç‰‡æ•°æ®ã€‚
    
    Args:
        image_path: å›¾ç‰‡æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„
        
    Returns:
        str: Base64ç¼–ç åçš„å›¾ç‰‡å­—ç¬¦ä¸²
    """
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')


def should_continue(state: AgentState):
    """åˆ¤æ–­å·¥ä½œæµæ˜¯å¦åº”è¯¥ç»§ç»­æ‰§è¡Œ
    
    æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨ï¼Œå†³å®šä¸‹ä¸€æ­¥æ˜¯æ‰§è¡Œå·¥å…·è¿˜æ˜¯ç»“æŸæµç¨‹ã€‚
    
    Args:
        state: AgentçŠ¶æ€å¯¹è±¡ï¼ŒåŒ…å«æ¶ˆæ¯å†å²
        
    Returns:
        str: "tools" è¡¨ç¤ºéœ€è¦æ‰§è¡Œå·¥å…·ï¼ŒEND è¡¨ç¤ºæµç¨‹ç»“æŸ
    """
    messages = state["messages"]
    last_message = messages[-1]
    # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨è¯·æ±‚
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    return END



# ========== å»å“ªåƒåŠŸèƒ½é€»è¾‘ ==========

async def where_to_eat_node(state: AgentState, config: RunnableConfig):
    """å¤„ç†"å»å“ªåƒ"åŠŸèƒ½çš„ä¸»èŠ‚ç‚¹ï¼Œè´Ÿè´£å›¾ç‰‡ä½ç½®è¯†åˆ«å’Œæµå¼è¾“å‡º
    
    è¯¥èŠ‚ç‚¹æ¥æ”¶ç”¨æˆ·ä¸Šä¼ çš„å›¾ç‰‡ï¼Œå…ˆè¿”å›é¢„è®¾å“åº”æ–‡æœ¬ç»™ç”¨æˆ·å³æ—¶åé¦ˆï¼Œ
    ç„¶åè°ƒç”¨LLMè¿›è¡Œä½ç½®æ¨ç†ã€‚ä½¿ç”¨ContentSplitterè§£æLLMè¾“å‡ºï¼Œ
    å°†æ€è€ƒè¿‡ç¨‹ï¼ˆreason-contentå—ï¼‰å’Œæœ€ç»ˆç­”æ¡ˆï¼ˆanswerå—ï¼‰åˆ†å¼€è¿”å›ã€‚
    
    Args:
        state: AgentçŠ¶æ€å¯¹è±¡ï¼ŒåŒ…å«å›¾ç‰‡è·¯å¾„å’Œæ¶ˆæ¯å†å²
        config: LangChainè¿è¡Œé…ç½®ï¼Œç”¨äºäº‹ä»¶æ´¾å‘
        
    Yields:
        dict: åŒ…å«æ¶ˆæ¯æ›´æ–°çš„çŠ¶æ€å­—å…¸
    """
    import httpx
    
    # ä»çŠ¶æ€ä¸­è·å–å›¾ç‰‡è·¯å¾„å’Œç”¨æˆ·æŸ¥è¯¢
    image_path = state.get("image_path")
    user_query = state.get("messages", [{}])[0].content if state.get("messages") else "è¿™æ˜¯å“ªé‡Œ?"
    
    # ========== æ­¥éª¤1: å‘é€é¢„è®¾å“åº” ==========
    # ç«‹å³è¿”å›é¢„è®¾å“åº”æ–‡æœ¬ï¼Œç»™ç”¨æˆ·å³æ—¶åé¦ˆï¼Œæå‡ä½“éªŒ
    preset_text = get_preset_response(WHERE_TO_EAT_PRESETS)
    await adispatch_custom_event("thought", {"content": preset_text}, config=config)
    
    # ========== æ­¥éª¤2: å¤„ç†å›¾ç‰‡ ==========
    # å°†å›¾ç‰‡è½¬æ¢ä¸ºBase64ç¼–ç æˆ–ä»URLä¸‹è½½
    if image_path.startswith("http"):
        # å›¾ç‰‡æ˜¯è¿œç¨‹URLï¼Œéœ€è¦ä¸‹è½½å¹¶ç¼–ç 
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(image_path, timeout=30.0)
                response.raise_for_status()
                image_data = base64.b64encode(response.content).decode("utf-8")
                content_type = response.headers.get("content-type", "image/jpeg")
                image_url = f"data:{content_type};base64,{image_data}"
        except Exception as e:
            # å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
            yield {"messages": [AIMessage(
                content=f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {str(e)}",
                additional_kwargs={"message": f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {str(e)}"}
            )]}
            return
    else:
        # å›¾ç‰‡æ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„
        if not os.path.exists(image_path):
            yield {"messages": [AIMessage(content="å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨")]}
            return
        base64_image = encode_image(image_path)
        image_url = f"data:image/jpeg;base64,{base64_image}"
    
    # ========== æ­¥éª¤3: è°ƒç”¨LLMè¿›è¡Œåˆ†æ ==========
    llm_config = settings.llm
    
    # æ³¨æ„ï¼šo4-mini ç­‰æ¨ç†æ¨¡å‹åªæ”¯æŒé»˜è®¤çš„ temperature=1ï¼Œä¸ä¼ å…¥è¯¥å‚æ•°
    model = ChatOpenAI(
        model=llm_config.default_model,
        base_url=llm_config.openai_api_base
    )
    
    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼šç³»ç»Ÿæç¤ºè¯ + ç”¨æˆ·æ¶ˆæ¯ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰
    messages = [
        SystemMessage(content=WHERE_TO_EAT_PROMPT),
        HumanMessage(content=[
            {"type": "text", "text": user_query},
            {"type": "image_url", "image_url": {"url": image_url}}
        ])
    ]
    
    # ========== æ­¥éª¤3.5: åˆå§‹åŒ–å†…å®¹åˆ†å‰²å™¨ ==========
    # ä½¿ç”¨ContentSplitterè§£æLLMè¾“å‡ºï¼Œæ ¹æ®``` reason-contentå’Œ``` answeræ ‡è®°åˆ†å‰²
    splitter = ContentSplitter()
    response_content = ""     # å®Œæ•´å“åº”å†…å®¹ç´¯åŠ 
    thought_content = ""      # æ¨ç†æ¨¡å‹ä¸“ç”¨å­—æ®µçš„æ€è€ƒå†…å®¹
    
    print(f"[DEBUG] å¼€å§‹å¤„ç†å»å“ªåƒè¯·æ±‚ï¼Œå›¾ç‰‡: {image_path}")
    
    try:
        # æµå¼è°ƒç”¨LLMï¼Œå®æ—¶è·å–å“åº”å†…å®¹
        async for chunk in model.astream(messages, config=config):
            chunk_content = ""  # å½“å‰ chunk çš„å†…å®¹
            
            # ===== ä¼˜å…ˆçº§1: å¤„ç†æ¨ç†å†…å®¹ï¼ˆä¸“ç”¨å­—æ®µï¼‰ =====
            # DeepSeek ç­‰æ¨ç†æ¨¡å‹ä¼šåœ¨ additional_kwargs.reasoning_content ä¸­è¿”å›æ¨ç†è¿‡ç¨‹
            if hasattr(chunk, 'additional_kwargs') and chunk.additional_kwargs:
                reasoning = chunk.additional_kwargs.get("reasoning_content", "")
                if reasoning:
                    # æ¨ç†å†…å®¹ç‹¬ç«‹å¤„ç†ï¼Œç›´æ¥ä½œä¸ºæ€è€ƒè¿‡ç¨‹æ´¾å‘
                    await adispatch_custom_event("thought", {"content": reasoning}, config=config)
                    thought_content += reasoning
                    continue  # æ¨ç†å†…å®¹å¤„ç†å®Œæ¯•ï¼Œè·³è¿‡åç»­é€»è¾‘
            
            # ===== ä¼˜å…ˆçº§2: å¤„ç† content_blocksï¼ˆç»“æ„åŒ–è¾“å‡ºï¼‰ =====
            has_content_blocks = False
            if hasattr(chunk, 'content_blocks') and chunk.content_blocks:
                has_content_blocks = True
                for block in chunk.content_blocks:
                    block_type = block.get("type", "")
                    
                    if block_type == "reasoning":
                        reasoning_text = block.get("reasoning", "")
                        if reasoning_text:
                            await adispatch_custom_event("thought", {"content": reasoning_text}, config=config)
                            thought_content += reasoning_text
                    
                    elif block_type == "text":
                        text_content = block.get("text", "")
                        if text_content:
                            chunk_content += text_content
            
            # ===== ä¼˜å…ˆçº§3: å¤„ç†æ™®é€š content å­—æ®µ =====
            if not has_content_blocks and chunk.content:
                chunk_content = chunk.content
            
            # ===== ä½¿ç”¨ContentSplitterè¿›è¡Œå†…å®¹åˆ†æµ =====
            if chunk_content:
                response_content += chunk_content
                
                # å¤„ç†chunkå¹¶è·å–éœ€è¦å‘å°„çš„äº‹ä»¶
                events = splitter.process_chunk(chunk_content)
                
                for event in events:
                    event_type = event["type"]
                    event_content = event["content"]
                    
                    if event_type == "thought":
                        # æ€è€ƒè¿‡ç¨‹ï¼šå‘é€thoughtäº‹ä»¶
                        await adispatch_custom_event("thought", {"content": event_content}, config=config)
                    elif event_type == "message":
                        # æœ€ç»ˆç­”æ¡ˆï¼šå‘é€messageäº‹ä»¶
                        # æ¸…ç†å¯èƒ½çš„ JSON æ®‹ç•™æ ‡è®°
                        clean_content = re.sub(r'```json\s*\{[^`]*?\}\s*```', '', event_content)
                        clean_content = re.sub(r'\s*"\s*\}\s*$', '', clean_content)
                        clean_content = re.sub(r'^\s*"\s*\}\s*', '', clean_content)
                        if clean_content.strip():
                            await adispatch_custom_event("message", {"content": clean_content}, config=config)
        
        # æµå¼ä¼ è¾“ç»“æŸï¼Œåˆ·æ–°ç¼“å†²åŒºä¸­çš„å‰©ä½™å†…å®¹
        flush_events = splitter.flush()
        for event in flush_events:
            event_type = event["type"]
            event_content = event["content"]
            
            if event_type == "thought":
                await adispatch_custom_event("thought", {"content": event_content}, config=config)
            elif event_type == "message":
                # æ¸…ç†å¯èƒ½çš„ JSON æ®‹ç•™æ ‡è®°
                clean_content = re.sub(r'```json\s*\{[^`]*?\}\s*```', '', event_content)
                clean_content = re.sub(r'\s*"\s*\}\s*$', '', clean_content)
                clean_content = re.sub(r'^\s*"\s*\}\s*', '', clean_content)
                if clean_content.strip():
                    await adispatch_custom_event("message", {"content": clean_content}, config=config)

    except Exception as e:
        # LLMè°ƒç”¨å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        error_msg = f"AIæœåŠ¡è°ƒç”¨å¤±è´¥: {str(e)}"
        yield {"messages": [AIMessage(
            content=error_msg,
            additional_kwargs={"message": error_msg}
        )]}
        return
    
    # ========== æ­¥éª¤4: è·å–è§£æç»“æœå¹¶æ‰“å°è°ƒè¯•ä¿¡æ¯ ==========
    parsed = splitter.get_parsed_content()
    
    print(f"\n{'='*60}")
    print("[DEBUG] LLM å®Œæ•´è¾“å‡ºå†…å®¹:")
    print(f"{'='*60}")
    print(f"æ€è€ƒè¿‡ç¨‹é•¿åº¦ (reason-content): {len(parsed.thought)}")
    print(f"æœ€ç»ˆç­”æ¡ˆé•¿åº¦ (answer): {len(parsed.answer)}")
    print(f"æ¨ç†æ¨¡å‹æ€è€ƒé•¿åº¦: {len(thought_content)}")
    print(f"æ€»å“åº”é•¿åº¦: {len(response_content)}")
    print(f"{'='*60}\n")
    
    # ========== æ­¥éª¤5: æå–ä½ç½®JSONä¿¡æ¯ ==========
    json_pattern = r'```json\s*(\{[^`]*?\})\s*```'
    json_matches = re.findall(json_pattern, response_content, re.DOTALL)
    
    locations = []
    for json_str in json_matches:
        try:
            json_data = json.loads(json_str)
            if json_data.get("latitude") and json_data.get("longitude"):
                locations.append(json_data)
        except json.JSONDecodeError:
            continue
    
    # ========== æ­¥éª¤6: æ„å»ºæœ€ç»ˆå“åº”æ¶ˆæ¯ ==========
    final_messages = []
    
    # åˆå¹¶æ€è€ƒè¿‡ç¨‹ï¼ˆæ¨ç†æ¨¡å‹çš„reasoning_content + reason-contentå—ï¼‰
    combined_thought = thought_content + parsed.thought
    if combined_thought:
        final_messages.append(AIMessage(
            content=combined_thought,
            additional_kwargs={"thought": combined_thought}
        ))

    # ä½¿ç”¨è§£æçš„answerä½œä¸ºæœ€ç»ˆç»“æœï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    result_content = parsed.answer if parsed.answer else response_content
    # ä»ç»“æœå†…å®¹ä¸­ç§»é™¤ JSON ä»£ç å—
    result_content = re.sub(r'```json\s*\{[^`]*?\}\s*```', '', result_content)
    # ç§»é™¤ JSON æ ·å¼å—çš„æ®‹ç•™ç»“æŸæ ‡è®° "}ï¼ˆå¯èƒ½åœ¨æœ«å°¾æˆ–å•ç‹¬ä¸€è¡Œï¼‰
    result_content = re.sub(r'\s*"\s*\}\s*$', '', result_content)
    result_content = re.sub(r'^\s*"\s*\}\s*', '', result_content)
    result_content = result_content.strip()
    
    if result_content:
        final_messages.append(AIMessage(
            content=result_content,
            additional_kwargs={"message": result_content}
        ))

    # æ·»åŠ ä½ç½®ä¿¡æ¯ï¼ˆæ”¯æŒå¤šä¸ªåº—é“ºï¼‰
    if locations:
        for loc in locations:
            final_messages.append(AIMessage(
                content=f"ä½ç½®å·²è¯†åˆ«: {loc.get('name', 'æœªçŸ¥åœ°ç‚¹')}",
                additional_kwargs={
                    "function_call": {
                        "action": "open_map",
                        "lat": loc.get("latitude"),
                        "lng": loc.get("longitude"),
                        "name": loc.get("name", "æœªçŸ¥åœ°ç‚¹"),
                        "address": loc.get("address", "åœ°å€æœªçŸ¥")
                    }
                }
            ))
    else:
        # æœªèƒ½æå–ä½ç½®ä¿¡æ¯
        final_messages.append(AIMessage(
            content="æœªèƒ½ä»å“åº”ä¸­æå–ä½ç½®ä¿¡æ¯ã€‚",
            additional_kwargs={"message": "æœªèƒ½ä»å“åº”ä¸­æå–ä½ç½®ä¿¡æ¯ã€‚"}
        ))
        
    yield {"messages": final_messages}


# æ„å»º"å»å“ªåƒ"å·¥ä½œæµå›¾
where_to_eat_workflow = StateGraph(AgentState)
where_to_eat_workflow.add_node("agent", where_to_eat_node)
where_to_eat_workflow.set_entry_point("agent")
where_to_eat_workflow.add_edge("agent", END)
where_to_eat_graph = where_to_eat_workflow.compile()




# ========== æŸ¥é¢„åˆ¶åŠŸèƒ½é€»è¾‘ ==========

# ========== æŸ¥é¢„åˆ¶åŠŸèƒ½é€»è¾‘ ==========

async def visual_analysis_node(state: AgentState, config: RunnableConfig):
    """è§†è§‰åˆ†æèŠ‚ç‚¹ï¼šåˆ†æç‰©ç†ç‰¹å¾"""
    from app.constants.prompts import VISUAL_ANALYSIS_PROMPT
    import httpx
    
    image_path = state.get("image_path")
    # ... Image processing (reuse duplicated code or refactor utils later) ...
    # For brevity, assuming image_path is handled (reuse logic or helper)
    
    # Helper for image url (Duplicate logic reduction recommended for production)
    image_url = ""
    if image_path.startswith("http"):
        async with httpx.AsyncClient() as client:
            response = await client.get(image_path, timeout=30.0)
            image_data = base64.b64encode(response.content).decode("utf-8")
            image_url = f"data:image/jpeg;base64,{image_data}"
    else:
        if os.path.exists(image_path):
            base64_image = encode_image(image_path)
            image_url = f"data:image/jpeg;base64,{base64_image}"
            
    llm_config = settings.llm
    model = ChatOpenAI(model=llm_config.default_model, base_url=llm_config.openai_api_base, api_key=llm_config.openai_api_key)
    
    messages = [
        SystemMessage(content=VISUAL_ANALYSIS_PROMPT),
        HumanMessage(content=[
            {"type": "text", "text": "åˆ†æè¿™å¼ å›¾ç‰‡"},
            {"type": "image_url", "image_url": {"url": image_url}}
        ])
    ]
    
    response = await model.ainvoke(messages)
    await adispatch_custom_event("thought", {"content": "æ­£åœ¨åˆ†æè§†è§‰ç‰¹å¾ï¼ˆè‰²æ³½ã€è´¨åœ°ï¼‰...\n"}, config=config)
    return {"visual_report": response.content}


async def process_analysis_node(state: AgentState, config: RunnableConfig):
    """å·¥è‰ºåˆ†æèŠ‚ç‚¹ï¼šåˆ†æå·¥ä¸šåŒ–ç—•è¿¹"""
    from app.constants.prompts import PROCESS_ANALYSIS_PROMPT
    import httpx
    
    image_path = state.get("image_path")
    # Helper for image url
    image_url = ""
    if image_path.startswith("http"):
        async with httpx.AsyncClient() as client:
            response = await client.get(image_path, timeout=30.0)
            image_data = base64.b64encode(response.content).decode("utf-8")
            image_url = f"data:image/jpeg;base64,{image_data}"
    else:
        if os.path.exists(image_path):
            base64_image = encode_image(image_path)
            image_url = f"data:image/jpeg;base64,{base64_image}"
            
    llm_config = settings.llm
    model = ChatOpenAI(model=llm_config.default_model, base_url=llm_config.openai_api_base, api_key=llm_config.openai_api_key)
    
    messages = [
        SystemMessage(content=PROCESS_ANALYSIS_PROMPT),
        HumanMessage(content=[
            {"type": "text", "text": "åˆ†æè¿™å¼ å›¾ç‰‡"},
            {"type": "image_url", "image_url": {"url": image_url}}
        ])
    ]
    
    response = await model.ainvoke(messages)
    await adispatch_custom_event("thought", {"content": "æ­£åœ¨æ¨æµ‹åˆ¶ä½œå·¥è‰ºï¼ˆé”…æ°”ã€å·¥ä¸šç—•è¿¹ï¼‰...\n"}, config=config)
    return {"process_report": response.content}


async def check_premade_aggregator_node(state: AgentState, config: RunnableConfig):
    """èšåˆèŠ‚ç‚¹ï¼šæ±‡æ€»åˆ†æå¹¶è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š"""
    from app.constants.prompts import CHECK_PREMADE_MAIN_PROMPT
    
    visual_report = state.get("visual_report", "æœªè·å–åˆ°è§†è§‰åˆ†æ")
    process_report = state.get("process_report", "æœªè·å–åˆ°å·¥è‰ºåˆ†æ")
    
    # Send preset thought
    preset_text = get_preset_response(CHECK_PREMADE_PRESETS)
    await adispatch_custom_event("thought", {"content": f"{preset_text}\næ­£åœ¨ç»¼åˆå¤šç»´åº¦åˆ†æç»“æœ..."}, config=config)
    
    llm_config = settings.llm
    model = ChatOpenAI(model=llm_config.default_model, base_url=llm_config.openai_api_base, api_key=llm_config.openai_api_key)
    
    messages = [
        SystemMessage(content=CHECK_PREMADE_MAIN_PROMPT),
        HumanMessage(content=f"ã€è§†è§‰åˆ†ææŠ¥å‘Šã€‘\n{visual_report}\n\nã€å·¥è‰ºåˆ†ææŠ¥å‘Šã€‘\n{process_report}")
    ]
    
    # Initialize splitter
    splitter = ContentSplitter()
    response_content = ""
    
    try:
        async for chunk in model.astream(messages, config=config):
            chunk_content = ""
            if chunk.content:
                chunk_content = chunk.content
                
            if chunk_content:
                response_content += chunk_content
                # Direct streaming to message event
                await adispatch_custom_event("message", {"content": chunk_content}, config=config)
                
    except Exception as e:
        error_msg = f"èšåˆåˆ†æå¤±è´¥: {str(e)}"
        return {"messages": [AIMessage(content=error_msg)]}

    # Final message construction (optional, as we streamed)
    return {"messages": [AIMessage(content=response_content)]}


# æ„å»º"æŸ¥é¢„åˆ¶"å·¥ä½œæµå›¾
premade_workflow = StateGraph(AgentState)

# Add nodes
premade_workflow.add_node("visual_analysis", visual_analysis_node)
premade_workflow.add_node("process_analysis", process_analysis_node)
premade_workflow.add_node("aggregator", check_premade_aggregator_node)

# Set entry point to broadcast to parallel nodes
premade_workflow.set_entry_point("visual_analysis") # LangGraph requires one entry, but we can fan out?
# Actually LangGraph entry point can be one node. To fan out, we usually have a 'start' node or just connect entry to multiple?
# Limitation: Entry point must be single node.
# Strategy: Add a dummy 'start' node that does nothing but pass state, OR just chain:
# Start -> (Visual | Process) -> Aggregator
# Let's add a simple router/start node to facilitate parallel execution properly.

async def start_node(state: AgentState):
    return {}

premade_workflow.add_node("start", start_node)
premade_workflow.set_entry_point("start")

# Fan out from start
premade_workflow.add_edge("start", "visual_analysis")
premade_workflow.add_edge("start", "process_analysis")

# Fan in to aggregator
premade_workflow.add_edge("visual_analysis", "aggregator")
premade_workflow.add_edge("process_analysis", "aggregator")

premade_workflow.add_edge("aggregator", END)

premade_graph = premade_workflow.compile()


# ========== åƒå¤šå°‘åŠŸèƒ½é€»è¾‘ ==========

# å¹¶å‘èŠ‚ç‚¹1ï¼šé£Ÿç‰©è¯†åˆ«
async def food_identification_node(state: AgentState, config: RunnableConfig):
    """é£Ÿç‰©è¯†åˆ«èŠ‚ç‚¹ï¼šè¯†åˆ«å›¾ç‰‡ä¸­çš„æ‰€æœ‰é£Ÿç‰©
    
    åˆ†æå›¾ç‰‡ä¸­çš„é£Ÿç‰©ç§ç±»ã€ä»½é‡ã€çƒ¹é¥ªæ–¹å¼ç­‰ä¿¡æ¯ã€‚
    
    Args:
        state: AgentçŠ¶æ€å¯¹è±¡
        config: LangChainè¿è¡Œé…ç½®
        
    Returns:
        dict: åŒ…å«é£Ÿç‰©è¯†åˆ«ç»“æœçš„çŠ¶æ€æ›´æ–°
    """
    from app.constants.prompts import FOOD_IDENTIFICATION_PROMPT
    import httpx
    
    image_path = state.get("image_path")
    
    # å¤„ç†å›¾ç‰‡URL
    image_url = ""
    if image_path.startswith("http"):
        async with httpx.AsyncClient() as client:
            response = await client.get(image_path, timeout=30.0)
            image_data = base64.b64encode(response.content).decode("utf-8")
            image_url = f"data:image/jpeg;base64,{image_data}"
    else:
        if os.path.exists(image_path):
            base64_image = encode_image(image_path)
            image_url = f"data:image/jpeg;base64,{base64_image}"
    
    llm_config = settings.llm
    model = ChatOpenAI(
        model=llm_config.default_model,
        base_url=llm_config.openai_api_base,
        api_key=llm_config.openai_api_key
    )
    
    messages = [
        SystemMessage(content=FOOD_IDENTIFICATION_PROMPT),
        HumanMessage(content=[
            {"type": "text", "text": "è¯·è¯†åˆ«è¿™å¼ å›¾ç‰‡ä¸­çš„æ‰€æœ‰é£Ÿç‰©"},
            {"type": "image_url", "image_url": {"url": image_url}}
        ])
    ]
    
    response = await model.ainvoke(messages)
    await adispatch_custom_event("thought", {"content": "ğŸ½ï¸ æ­£åœ¨è¯†åˆ«å›¾ç‰‡ä¸­çš„é£Ÿç‰©...\n"}, config=config)
    return {"food_report": response.content}


# å¹¶å‘èŠ‚ç‚¹2ï¼šçƒ­é‡ä¼°ç®—
async def calorie_estimation_node(state: AgentState, config: RunnableConfig):
    """çƒ­é‡ä¼°ç®—èŠ‚ç‚¹ï¼šä¼°ç®—æ¯ç§é£Ÿç‰©çš„çƒ­é‡
    
    æ ¹æ®é£Ÿç‰©ç§ç±»å’Œä»½é‡è®¡ç®—çƒ­é‡å€¼ã€‚
    
    Args:
        state: AgentçŠ¶æ€å¯¹è±¡
        config: LangChainè¿è¡Œé…ç½®
        
    Returns:
        dict: åŒ…å«çƒ­é‡ä¼°ç®—ç»“æœçš„çŠ¶æ€æ›´æ–°
    """
    from app.constants.prompts import CALORIE_ESTIMATION_PROMPT
    import httpx
    
    image_path = state.get("image_path")
    
    # å¤„ç†å›¾ç‰‡URL
    image_url = ""
    if image_path.startswith("http"):
        async with httpx.AsyncClient() as client:
            response = await client.get(image_path, timeout=30.0)
            image_data = base64.b64encode(response.content).decode("utf-8")
            image_url = f"data:image/jpeg;base64,{image_data}"
    else:
        if os.path.exists(image_path):
            base64_image = encode_image(image_path)
            image_url = f"data:image/jpeg;base64,{base64_image}"
    
    llm_config = settings.llm
    model = ChatOpenAI(
        model=llm_config.default_model,
        base_url=llm_config.openai_api_base,
        api_key=llm_config.openai_api_key
    )
    
    messages = [
        SystemMessage(content=CALORIE_ESTIMATION_PROMPT),
        HumanMessage(content=[
            {"type": "text", "text": "è¯·ä¼°ç®—å›¾ç‰‡ä¸­æ¯ç§é£Ÿç‰©çš„çƒ­é‡"},
            {"type": "image_url", "image_url": {"url": image_url}}
        ])
    ]
    
    response = await model.ainvoke(messages)
    await adispatch_custom_event("thought", {"content": "ğŸ”¢ æ­£åœ¨ä¼°ç®—é£Ÿç‰©çƒ­é‡...\n"}, config=config)
    return {"calorie_report": response.content}


# å¹¶å‘èŠ‚ç‚¹3ï¼šè¿åŠ¨æ¶ˆè€—ä¼°ç®—
async def exercise_estimation_node(state: AgentState, config: RunnableConfig):
    """è¿åŠ¨æ¶ˆè€—ä¼°ç®—èŠ‚ç‚¹ï¼šè®¡ç®—æ¶ˆè€—çƒ­é‡æ‰€éœ€çš„è¿åŠ¨é‡
    
    å°†çƒ­é‡è½¬æ¢ä¸ºå…·ä½“çš„è¿åŠ¨æ—¶é—´å»ºè®®ã€‚
    
    Args:
        state: AgentçŠ¶æ€å¯¹è±¡
        config: LangChainè¿è¡Œé…ç½®
        
    Returns:
        dict: åŒ…å«è¿åŠ¨æ¶ˆè€—ç»“æœçš„çŠ¶æ€æ›´æ–°
    """
    from app.constants.prompts import EXERCISE_ESTIMATION_PROMPT
    import httpx
    
    image_path = state.get("image_path")
    
    # å¤„ç†å›¾ç‰‡URL
    image_url = ""
    if image_path.startswith("http"):
        async with httpx.AsyncClient() as client:
            response = await client.get(image_path, timeout=30.0)
            image_data = base64.b64encode(response.content).decode("utf-8")
            image_url = f"data:image/jpeg;base64,{image_data}"
    else:
        if os.path.exists(image_path):
            base64_image = encode_image(image_path)
            image_url = f"data:image/jpeg;base64,{base64_image}"
    
    llm_config = settings.llm
    model = ChatOpenAI(
        model=llm_config.default_model,
        base_url=llm_config.openai_api_base,
        api_key=llm_config.openai_api_key
    )
    
    messages = [
        SystemMessage(content=EXERCISE_ESTIMATION_PROMPT),
        HumanMessage(content=[
            {"type": "text", "text": "è¯·è®¡ç®—æ¶ˆè€—è¿™äº›é£Ÿç‰©çƒ­é‡æ‰€éœ€çš„è¿åŠ¨é‡"},
            {"type": "image_url", "image_url": {"url": image_url}}
        ])
    ]
    
    response = await model.ainvoke(messages)
    await adispatch_custom_event("thought", {"content": "ğŸƒ æ­£åœ¨è®¡ç®—è¿åŠ¨æ¶ˆè€—...\n"}, config=config)
    return {"exercise_report": response.content}


# èšåˆèŠ‚ç‚¹ï¼šæ±‡æ€»åˆ†æå¹¶è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
async def calories_aggregator_node(state: AgentState, config: RunnableConfig):
    """èšåˆèŠ‚ç‚¹ï¼šæ±‡æ€»æ‰€æœ‰åˆ†æç»“æœå¹¶ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    
    æ ¹æ®é£Ÿç‰©è¯†åˆ«ã€çƒ­é‡ä¼°ç®—ã€è¿åŠ¨æ¶ˆè€—æŠ¥å‘Šç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Šã€‚
    
    Args:
        state: AgentçŠ¶æ€å¯¹è±¡ï¼ŒåŒ…å«å„èŠ‚ç‚¹åˆ†æç»“æœ
        config: LangChainè¿è¡Œé…ç½®
        
    Returns:
        dict: åŒ…å«æœ€ç»ˆæŠ¥å‘Šçš„çŠ¶æ€æ›´æ–°
    """
    from app.constants.prompts import CALORIES_MAIN_PROMPT
    
    food_report = state.get("food_report", "æœªè·å–åˆ°é£Ÿç‰©è¯†åˆ«æŠ¥å‘Š")
    calorie_report = state.get("calorie_report", "æœªè·å–åˆ°çƒ­é‡ä¼°ç®—æŠ¥å‘Š")
    exercise_report = state.get("exercise_report", "æœªè·å–åˆ°è¿åŠ¨æ¶ˆè€—æŠ¥å‘Š")
    meal_time = state.get("meal_time", "åˆé¤")
    
    # å‘é€é¢„è®¾æ€è€ƒ
    preset_text = get_preset_response(CALORIES_PRESETS)
    await adispatch_custom_event("thought", {"content": f"{preset_text}\nâ° æ­£åœ¨ç»¼åˆåˆ†æç»“æœ..."}, config=config)
    
    llm_config = settings.llm
    model = ChatOpenAI(
        model=llm_config.default_model,
        base_url=llm_config.openai_api_base,
        api_key=llm_config.openai_api_key
    )
    
    # å¡«å……æç¤ºè¯ä¸­çš„meal_time
    prompt = CALORIES_MAIN_PROMPT.replace("{meal_time}", meal_time)
    
    messages = [
        SystemMessage(content=prompt),
        HumanMessage(content=f"""ã€é£Ÿç‰©è¯†åˆ«æŠ¥å‘Šã€‘
{food_report}

ã€çƒ­é‡ä¼°ç®—æŠ¥å‘Šã€‘
{calorie_report}

ã€è¿åŠ¨æ¶ˆè€—æŠ¥å‘Šã€‘
{exercise_report}

ç”¨é¤æ—¶é—´ï¼š{meal_time}

è¯·æ ¹æ®ä»¥ä¸ŠæŠ¥å‘Šç”Ÿæˆç»¼åˆåˆ†æç»“æœã€‚""")
    ]
    
    # åˆå§‹åŒ–å†…å®¹åˆ†å‰²å™¨
    splitter = ContentSplitter()
    response_content = ""
    
    try:
        async for chunk in model.astream(messages, config=config):
            chunk_content = ""
            if chunk.content:
                chunk_content = chunk.content
            
            if chunk_content:
                response_content += chunk_content
                # ä½¿ç”¨ContentSplitterè¿›è¡Œå†…å®¹åˆ†æµ
                events = splitter.process_chunk(chunk_content)
                
                for event in events:
                    event_type = event["type"]
                    event_content = event["content"]
                    
                    if event_type == "thought":
                        await adispatch_custom_event("thought", {"content": event_content}, config=config)
                    elif event_type == "message":
                        await adispatch_custom_event("message", {"content": event_content}, config=config)
        
        # åˆ·æ–°ç¼“å†²åŒº
        flush_events = splitter.flush()
        for event in flush_events:
            event_type = event["type"]
            event_content = event["content"]
            
            if event_type == "thought":
                await adispatch_custom_event("thought", {"content": event_content}, config=config)
            elif event_type == "message":
                await adispatch_custom_event("message", {"content": event_content}, config=config)
                
    except Exception as e:
        error_msg = f"èšåˆåˆ†æå¤±è´¥: {str(e)}"
        return {"messages": [AIMessage(content=error_msg)]}
    
    # è§£æJSONç»“æœå¹¶ç”Ÿæˆfunction_call
    try:
        # å°è¯•æå–JSON
        json_match = re.search(r'\{[\s\S]*?"food_items"[\s\S]*?\}', response_content)
        if json_match:
            json_str = json_match.group()
            # æ¸…ç†JSONå­—ç¬¦ä¸²
            json_str = re.sub(r'"reason-content"\s*:\s*"[^"]*"\s*,?', '', json_str)
            json_str = re.sub(r'"answer"\s*:\s*"[^"]*"\s*,?', '', json_str)
            
            food_data = json.loads(json_str)
            
            # å‘é€function_calläº‹ä»¶
            await adispatch_custom_event("function_call", {
                "content": json.dumps({
                    "action": "calories_result",
                    "food_items": food_data.get("food_items", []),
                    "total_calories": food_data.get("total_calories", 0),
                    "overall_advice": food_data.get("overall_advice", "")
                })
            }, config=config)
    except Exception as e:
        print(f"[DEBUG] JSONè§£æå¤±è´¥: {e}")
    
    return {"messages": [AIMessage(content=response_content)]}


# æ„å»º"åƒå¤šå°‘"å¹¶å‘å·¥ä½œæµå›¾
calories_workflow = StateGraph(AgentState)

# æ·»åŠ å¯åŠ¨èŠ‚ç‚¹
async def calories_start_node(state: AgentState):
    """å¯åŠ¨èŠ‚ç‚¹ï¼šåˆå§‹åŒ–å·¥ä½œæµ"""
    return {}

calories_workflow.add_node("start", calories_start_node)
calories_workflow.add_node("food_identification", food_identification_node)
calories_workflow.add_node("calorie_estimation", calorie_estimation_node)
calories_workflow.add_node("exercise_estimation", exercise_estimation_node)
calories_workflow.add_node("aggregator", calories_aggregator_node)

# è®¾ç½®å…¥å£ç‚¹
calories_workflow.set_entry_point("start")

# ä»å¯åŠ¨èŠ‚ç‚¹å¹¶è¡Œæ‰§è¡Œä¸‰ä¸ªåˆ†æèŠ‚ç‚¹
calories_workflow.add_edge("start", "food_identification")
calories_workflow.add_edge("start", "calorie_estimation")
calories_workflow.add_edge("start", "exercise_estimation")

# æ±‡èšåˆ°èšåˆèŠ‚ç‚¹
calories_workflow.add_edge("food_identification", "aggregator")
calories_workflow.add_edge("calorie_estimation", "aggregator")
calories_workflow.add_edge("exercise_estimation", "aggregator")

calories_workflow.add_edge("aggregator", END)

calories_graph = calories_workflow.compile()
